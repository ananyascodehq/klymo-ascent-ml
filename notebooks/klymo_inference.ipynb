{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c3c47e",
   "metadata": {},
   "source": [
    "# ðŸ›°ï¸ KLYMO ASCENT ML - Satellite Super-Resolution\n",
    "\n",
    "**Geospatial Super-Resolution Pipeline with Hallucination Prevention**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- 8x super-resolution (10m â†’ 1.25m/pixel)\n",
    "- ESRGAN with Geospatial Attention Module (GAM)\n",
    "- Spatial Consistency Loss for hallucination prevention\n",
    "- Quality metrics: PSNR, SSIM, Edge Coherence\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/klymo-ascent-ml/blob/main/notebooks/klymo_inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd96c9",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install torch torchvision --quiet\n",
    "!pip install opencv-python pillow scikit-image matplotlib tqdm --quiet\n",
    "\n",
    "# Clone repository (if running on Colab)\n",
    "import os\n",
    "if not os.path.exists('klymo-ascent-ml'):\n",
    "    !git clone https://github.com/your-repo/klymo-ascent-ml.git\n",
    "    %cd klymo-ascent-ml\n",
    "else:\n",
    "    %cd klymo-ascent-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca907916",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_generator\n",
    "\n",
    "# Create generator\n",
    "generator = create_generator(\n",
    "    use_gam=False,  # Use RGB-only model for simplicity\n",
    "    in_channels=3,\n",
    "    num_rrdb_blocks=23,\n",
    "    scale_factor=8\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint (if available)\n",
    "checkpoint_path = 'checkpoints/best_model.pth'\n",
    "\n",
    "import os\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    print(f\"âœ“ Loaded checkpoint: {checkpoint_path}\")\n",
    "    if 'psnr' in checkpoint:\n",
    "        print(f\"  Checkpoint PSNR: {checkpoint['psnr']:.2f} dB\")\n",
    "else:\n",
    "    print(\"âš  No checkpoint found. Using untrained model for demo.\")\n",
    "\n",
    "generator.eval()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in generator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e6600",
   "metadata": {},
   "source": [
    "## 3. Create Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_satellite_image(size=64):\n",
    "    \"\"\"Create a synthetic satellite-like image.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    image = np.random.rand(size, size, 3) * 0.3 + 0.2\n",
    "    \n",
    "    # Add buildings\n",
    "    for _ in range(np.random.randint(5, 15)):\n",
    "        x = np.random.randint(0, size - 15)\n",
    "        y = np.random.randint(0, size - 15)\n",
    "        w = np.random.randint(5, 15)\n",
    "        h = np.random.randint(5, 15)\n",
    "        color = np.random.rand(3) * 0.3 + 0.4\n",
    "        image[y:y+h, x:x+w] = color\n",
    "    \n",
    "    # Add roads\n",
    "    for _ in range(np.random.randint(2, 4)):\n",
    "        if np.random.rand() > 0.5:\n",
    "            y = np.random.randint(0, size)\n",
    "            image[max(0, y-1):min(size, y+1), :] = [0.3, 0.3, 0.35]\n",
    "        else:\n",
    "            x = np.random.randint(0, size)\n",
    "            image[:, max(0, x-1):min(size, x+1)] = [0.3, 0.3, 0.35]\n",
    "    \n",
    "    # Add vegetation\n",
    "    for _ in range(np.random.randint(2, 5)):\n",
    "        x = np.random.randint(0, size - 10)\n",
    "        y = np.random.randint(0, size - 10)\n",
    "        r = np.random.randint(3, 7)\n",
    "        yy, xx = np.ogrid[:size, :size]\n",
    "        mask = ((xx - x - r) ** 2 + (yy - y - r) ** 2) < r ** 2\n",
    "        image[mask] = [0.2, 0.5 + np.random.rand() * 0.2, 0.15]\n",
    "    \n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "# Create sample LR image\n",
    "lr_image = create_sample_satellite_image(64)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(lr_image)\n",
    "plt.title(f'Low Resolution Input ({lr_image.shape[1]}x{lr_image.shape[0]})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fced32c",
   "metadata": {},
   "source": [
    "## 4. Super-Resolution Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded121ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_image(model, lr_image, device='cuda'):\n",
    "    \"\"\"Apply super-resolution to an image.\"\"\"\n",
    "    # Preprocess\n",
    "    lr_tensor = torch.from_numpy(lr_image).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    lr_tensor = lr_tensor.to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        sr_tensor = model(lr_tensor)\n",
    "    \n",
    "    # Postprocess\n",
    "    sr_tensor = torch.clamp(sr_tensor, 0, 1)\n",
    "    sr_image = sr_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    sr_image = (sr_image * 255).astype(np.uint8)\n",
    "    \n",
    "    return sr_image\n",
    "\n",
    "# Generate SR output\n",
    "print(\"Generating super-resolved image...\")\n",
    "sr_image = enhance_image(generator, lr_image, device)\n",
    "print(f\"SR output shape: {sr_image.shape}\")\n",
    "\n",
    "# Generate bicubic baseline\n",
    "lr_tensor = torch.from_numpy(lr_image).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "bicubic_tensor = F.interpolate(lr_tensor, scale_factor=8, mode='bicubic', align_corners=False)\n",
    "bicubic_image = (bicubic_tensor.squeeze(0).permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "bicubic_image = np.clip(bicubic_image, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d241f02",
   "metadata": {},
   "source": [
    "## 5. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24184dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# LR (upscaled for display)\n",
    "lr_display = cv2.resize(lr_image, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "axes[0].imshow(lr_display)\n",
    "axes[0].set_title(f'Low Resolution\\n({lr_image.shape[1]}x{lr_image.shape[0]} â†’ display)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Bicubic\n",
    "axes[1].imshow(bicubic_image)\n",
    "axes[1].set_title(f'Bicubic 8x\\n({bicubic_image.shape[1]}x{bicubic_image.shape[0]})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# SR\n",
    "axes[2].imshow(sr_image)\n",
    "axes[2].set_title(f'Super-Resolved 8x\\n({sr_image.shape[1]}x{sr_image.shape[0]})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c68f9",
   "metadata": {},
   "source": [
    "## 6. Zoom Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom into a region\n",
    "zoom_x, zoom_y = 128, 128\n",
    "zoom_size = 128\n",
    "\n",
    "bicubic_crop = bicubic_image[zoom_y:zoom_y+zoom_size, zoom_x:zoom_x+zoom_size]\n",
    "sr_crop = sr_image[zoom_y:zoom_y+zoom_size, zoom_x:zoom_x+zoom_size]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(bicubic_crop)\n",
    "axes[0].set_title('Bicubic (Zoomed 4x)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sr_crop)\n",
    "axes[1].set_title('Super-Resolved (Zoomed 4x)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d6710",
   "metadata": {},
   "source": [
    "## 7. Difference Map (Hallucination Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626832fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute difference map\n",
    "diff = np.abs(sr_image.astype(np.float32) - bicubic_image.astype(np.float32))\n",
    "diff_gray = np.mean(diff, axis=2)\n",
    "diff_normalized = (diff_gray / diff_gray.max() * 255).astype(np.uint8)\n",
    "\n",
    "# Apply colormap\n",
    "diff_heatmap = cv2.applyColorMap(diff_normalized, cv2.COLORMAP_JET)\n",
    "diff_heatmap = cv2.cvtColor(diff_heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(sr_image)\n",
    "axes[0].set_title('Super-Resolved Output')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(diff_heatmap)\n",
    "axes[1].set_title('Difference Map (SR vs Bicubic)\\nHot = High difference')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The difference map shows where SR added details.\")\n",
    "print(\"High values in unexpected areas may indicate hallucination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ed71f",
   "metadata": {},
   "source": [
    "## 8. Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# PSNR\n",
    "psnr = peak_signal_noise_ratio(bicubic_image, sr_image, data_range=255)\n",
    "print(f\"PSNR: {psnr:.2f} dB\")\n",
    "\n",
    "# SSIM\n",
    "ssim = structural_similarity(bicubic_image, sr_image, data_range=255, channel_axis=2)\n",
    "print(f\"SSIM: {ssim:.4f}\")\n",
    "\n",
    "# Edge Coherence\n",
    "sr_gray = cv2.cvtColor(sr_image, cv2.COLOR_RGB2GRAY)\n",
    "bicubic_gray = cv2.cvtColor(bicubic_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "sr_edges = cv2.Canny(sr_gray, 50, 150).flatten().astype(float)\n",
    "bicubic_edges = cv2.Canny(bicubic_gray, 50, 150).flatten().astype(float)\n",
    "\n",
    "if sr_edges.std() > 0 and bicubic_edges.std() > 0:\n",
    "    edge_coherence = np.corrcoef(sr_edges, bicubic_edges)[0, 1]\n",
    "    edge_coherence = max(0, edge_coherence) if not np.isnan(edge_coherence) else 0\n",
    "else:\n",
    "    edge_coherence = 1.0\n",
    "\n",
    "print(f\"Edge Coherence: {edge_coherence:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(f\"PSNR: {'âœ“ Good' if psnr > 25 else 'âš  Low'} (target: >28 dB)\")\n",
    "print(f\"SSIM: {'âœ“ Good' if ssim > 0.85 else 'âš  Fair' if ssim > 0.8 else 'âœ— Low'} (target: >0.85)\")\n",
    "print(f\"Edge Coherence: {'âœ“ No major hallucination' if edge_coherence > 0.85 else 'âš  Check for hallucination'} (target: >0.90)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46992e14",
   "metadata": {},
   "source": [
    "## 9. Edge Overlay Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e95bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_overlay(image, edge_color=(255, 0, 0), alpha=0.5):\n",
    "    \"\"\"Create edge overlay visualization.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    overlay = image.copy()\n",
    "    overlay[edges > 0] = edge_color\n",
    "    \n",
    "    result = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)\n",
    "    return result\n",
    "\n",
    "# Create edge overlays\n",
    "bicubic_edges_overlay = create_edge_overlay(bicubic_image)\n",
    "sr_edges_overlay = create_edge_overlay(sr_image)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(bicubic_edges_overlay)\n",
    "axes[0].set_title('Bicubic - Edge Overlay')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sr_edges_overlay)\n",
    "axes[1].set_title('Super-Resolved - Edge Overlay')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Edge overlay shows detected edges in red.\")\n",
    "print(\"SR should have similar edge locations as bicubic (no hallucinated edges).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae1048",
   "metadata": {},
   "source": [
    "## 10. Upload Your Own Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload image (works in Colab/Jupyter)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Upload a low-resolution satellite image:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        custom_image = np.array(Image.open(filename).convert('RGB'))\n",
    "        \n",
    "        # Resize if too large\n",
    "        max_size = 128\n",
    "        if max(custom_image.shape[:2]) > max_size:\n",
    "            scale = max_size / max(custom_image.shape[:2])\n",
    "            new_size = (int(custom_image.shape[1] * scale), int(custom_image.shape[0] * scale))\n",
    "            custom_image = cv2.resize(custom_image, new_size)\n",
    "        \n",
    "        print(f\"Image loaded: {custom_image.shape}\")\n",
    "        \n",
    "        # Enhance\n",
    "        custom_sr = enhance_image(generator, custom_image, device)\n",
    "        \n",
    "        # Display\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        axes[0].imshow(custom_image)\n",
    "        axes[0].set_title('Input')\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(custom_sr)\n",
    "        axes[1].set_title('Super-Resolved 8x')\n",
    "        axes[1].axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"File upload not available in this environment.\")\n",
    "    print(\"To use your own image, place it in the working directory and load with:\")\n",
    "    print(\"  custom_image = np.array(Image.open('your_image.png').convert('RGB'))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216aa585",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "This notebook demonstrated the KLYMO ASCENT ML super-resolution pipeline:\n",
    "\n",
    "1. **8x Super-Resolution**: 10m â†’ 1.25m/pixel\n",
    "2. **ESRGAN Architecture**: Enhanced with RRDB blocks\n",
    "3. **Hallucination Prevention**: Spatial Consistency Loss + Edge Coherence metric\n",
    "4. **Quality Metrics**: PSNR, SSIM, Edge Coherence\n",
    "\n",
    "### Key Takeaways:\n",
    "- **PSNR >28 dB**: Good reconstruction quality\n",
    "- **SSIM >0.85**: Good perceptual similarity\n",
    "- **Edge Coherence >0.90**: No major hallucination\n",
    "\n",
    "### Next Steps:\n",
    "- Train on real Sentinel-2/WorldView pairs\n",
    "- Enable Geospatial Attention Module (GAM)\n",
    "- Deploy to Hugging Face Spaces"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
